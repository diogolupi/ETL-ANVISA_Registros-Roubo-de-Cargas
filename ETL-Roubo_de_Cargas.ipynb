{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3141a3df-9182-4924-9bfe-869afed4fba3",
   "metadata": {},
   "source": [
    "\n",
    "## üì¶ Instala√ß√£o das Depend√™ncias\n",
    "\n",
    "Execute o bloco abaixo para instalar todas as bibliotecas necess√°rias para o pipeline:\n",
    "\n",
    "```python\n",
    "# Bibliotecas principais para o pipeline ETL\n",
    "!pip install pandas openpyxl pdfplumber camelot-py PyPDF2 beautifulsoup4 requests selenium webdriver-manager\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Explica√ß√£o r√°pida dos pacotes:\n",
    "\n",
    "- `pandas`: manipula√ß√£o de dados tabulares.\n",
    "- `openpyxl`: leitura e escrita de arquivos `.xlsx`.\n",
    "- `pdfplumber`: extra√ß√£o de tabelas de arquivos PDF com estrutura simples.\n",
    "- `camelot-py`: extra√ß√£o de tabelas estruturadas de arquivos PDF (requer Ghostscript).\n",
    "- `PyPDF2`: leitura e manipula√ß√£o de arquivos PDF, como metadados e p√°ginas.\n",
    "- `beautifulsoup4`: parser HTML para extra√ß√£o de links e conte√∫dos do site da ANVISA.\n",
    "- `requests`: download de p√°ginas e arquivos da web.\n",
    "- `selenium`: automa√ß√£o de navegador para lidar com conte√∫do din√¢mico (JavaScript).\n",
    "- `webdriver-manager`: instala e gerencia automaticamente o ChromeDriver necess√°rio para o Selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fddad0-f90a-4c40-aa64-f399aee19060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas principais para o pipeline ETL\n",
    "!pip install pandas openpyxl pdfplumber camelot-py PyPDF2 beautifulsoup4 requests selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b8ea9-97b0-46ab-89ec-f11b81332610",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è √çndice do Pipeline ETL - Roubos, Furtos e Extravios\n",
    "\n",
    "1. [Etapa 1 - Coleta de Dados: Roubos, Furtos e Extravios no site da ANVISA](#etapa-1---coleta-de-dados-roubos-furtos-e-extravios-no-site-da-anvisa)\n",
    "2. [Etapa 2 - Extra√ß√£o de Tabelas dos PDFs com `pdfplumber`](#etapa-2---extra√ß√£o-de-tabelas-dos-pdfs-com-pdfplumber)\n",
    "3. [Etapa 3 - Tratamento e Padroniza√ß√£o dos Arquivos Excel](#etapa-3---tratamento-e-padroniza√ß√£o-dos-arquivos-excel)\n",
    "4. [Etapa 4 - Consolida√ß√£o Final dos Dados](#etapa-4---consolida√ß√£o-final-dos-dados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb9bc9-07bc-44f8-817f-ff6dda1f013f",
   "metadata": {},
   "source": [
    "## üîç Etapa 1 - Coleta de Dados: Roubos, Furtos e Extravios no site da ANVISA\n",
    "\n",
    "Nesta etapa, realizamos a **extra√ß√£o automatizada de arquivos** contendo registros de roubos, furtos e extravios de produtos sujeitos √† vigil√¢ncia sanit√°ria, diretamente do portal da ANVISA:\n",
    "\n",
    "üîó **Fonte oficial:** [Portal ANVISA - Fiscaliza√ß√£o e Monitoramento](https://www.gov.br/anvisa/pt-br/assuntos/fiscalizacao-e-monitoramento/roubos-furtos-e-extravios)\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Objetivo\n",
    "\n",
    "Automatizar o processo de **extra√ß√£o e download** dos arquivos publicados no site da ANVISA, organizando-os por tipo de produto e ano. Os dados coletados s√£o essenciais para alimentar a pr√≥xima etapa do processo ETL.\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ Ferramentas Utilizadas\n",
    "\n",
    "- **Selenium**: renderiza o conte√∫do din√¢mico da p√°gina (JavaScript).\n",
    "- **BeautifulSoup**: realiza o parse do HTML e extrai os links relevantes.\n",
    "- **Requests**: efetua o download dos arquivos encontrados.\n",
    "- **Regex (re)**: identifica o tipo do produto e o ano a partir dos nomes dos links.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Organiza√ß√£o dos dados baixados\n",
    "\n",
    "Os arquivos s√£o organizados da seguinte forma:\n",
    "\n",
    "```\n",
    "dados/\n",
    "‚îú‚îÄ‚îÄ 2014/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ medicamentos_2014.xlsx\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ cosmeticos_2014.pdf\n",
    "‚îú‚îÄ‚îÄ 2015/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ produtos_saude_2015.xlsx\n",
    "‚îú‚îÄ‚îÄ 2018/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ medicamentos_2018.xlsx  ‚Üê adicionados manualmente\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß† L√≥gica do Script\n",
    "\n",
    "1. **Acessa a URL base** com Selenium e espera o carregamento da p√°gina.\n",
    "2. **Coleta todos os links** do HTML com BeautifulSoup.\n",
    "3. **Filtra os links** que cont√™m:\n",
    "   - Um ano (ex: `2019`)\n",
    "   - Um dos tipos de produto mapeados:\n",
    "     - medicamentos\n",
    "     - produtos para sa√∫de\n",
    "     - cosm√©ticos\n",
    "     - alimentos\n",
    "     - carga\n",
    "4. **Determina o tipo de produto e extens√£o do arquivo** (.pdf, .xlsx etc.).\n",
    "5. **Cria pastas por ano** (se necess√°rio) e salva o arquivo no caminho correspondente.\n",
    "6. **Registra o sucesso ou falha** de cada download com mensagens de log.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Resultado Esperado\n",
    "\n",
    "Ao final da execu√ß√£o, teremos um conjunto de arquivos .pdf ou .xlsx organizados por ano e tipo de produto, prontos para o processamento na pr√≥xima etapa do pipeline ETL (extra√ß√£o de dados tabulares, limpeza, padroniza√ß√£o e consolida√ß√£o).\n",
    "\n",
    "---\n",
    "\n",
    "### üõ°Ô∏è Observa√ß√µes\n",
    "\n",
    "- O script trata exce√ß√µes e links inv√°lidos.\n",
    "- URLs com extens√£o amb√≠gua s√£o tratadas como PDF por padr√£o.\n",
    "- O uso do `webdriver-manager` facilita a instala√ß√£o do ChromeDriver automaticamente.\n",
    "\n",
    "> ‚ö†Ô∏è **Importante:** Os arquivos referentes ao **ano de 2018** n√£o seguem o mesmo padr√£o de nomea√ß√£o ou estrutura de links automatiz√°veis. Portanto, eles devem ser **baixados manualmente do site**, **renomeados conforme os demais arquivos** (ex: `medicamentos_2018.xlsx`) e salvos na pasta:\n",
    ">\n",
    "> ```\n",
    "> dados/2018/\n",
    "> ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff566c27-a393-49c3-a9e0-6da6c3a5a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# === CONFIGURA√á√ÉO ===\n",
    "URL_BASE = \"https://www.gov.br/anvisa/pt-br/assuntos/fiscalizacao-e-monitoramento/roubos-furtos-e-extravios\"\n",
    "PASTA_DADOS = \"dados\"\n",
    "os.makedirs(PASTA_DADOS, exist_ok=True)\n",
    "\n",
    "TIPOS_PRODUTO = {\n",
    "    \"medicamentos\": \"medicamentos\",\n",
    "    \"produtos para a sa√∫de\": \"produtos_saude\",\n",
    "    \"produtos para sa√∫de\": \"produtos_saude\",\n",
    "    \"cosm√©ticos\": \"cosmeticos\",\n",
    "    \"alimentos\": \"alimentos\",\n",
    "    \"carga\": \"carga\"  # Adicionado para capturar planilhas como \"Roubo de Carga\"\n",
    "}\n",
    "\n",
    "# === ABRIR P√ÅGINA COM SELENIUM ===\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.get(URL_BASE)\n",
    "time.sleep(3)  # Espera o conte√∫do carregar\n",
    "\n",
    "html = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# === PARSE COM BEAUTIFULSOUP ===\n",
    "sopa = BeautifulSoup(html, \"html.parser\")\n",
    "links = sopa.find_all(\"a\", href=True)\n",
    "\n",
    "# === PROCESSAR LINKS ===\n",
    "for link in links:\n",
    "    texto = link.get_text(strip=True).lower()\n",
    "    href = link[\"href\"]\n",
    "\n",
    "    # Detectar ano\n",
    "    ano_match = re.search(r\"(\\d{4})\", texto)\n",
    "    if not ano_match:\n",
    "        continue\n",
    "    ano = ano_match.group(1)\n",
    "\n",
    "    # Detectar tipo de produto\n",
    "    tipo_match = next((TIPOS_PRODUTO[tipo] for tipo in TIPOS_PRODUTO if tipo in texto), None)\n",
    "    if not tipo_match:\n",
    "        continue\n",
    "\n",
    "    # Detectar extens√£o real\n",
    "    extensao = href.split(\".\")[-1].split(\"?\")[0].lower()\n",
    "    if len(extensao) > 5 or \"/\" in extensao:\n",
    "        extensao = \"pdf\"  # For√ßa PDF apenas se extens√£o for estranha\n",
    "\n",
    "    # Nome do arquivo\n",
    "    nome_arquivo = f\"{tipo_match}_{ano}.{extensao}\"\n",
    "    pasta_ano = os.path.join(PASTA_DADOS, ano)\n",
    "    os.makedirs(pasta_ano, exist_ok=True)\n",
    "    caminho_arquivo = os.path.join(pasta_ano, nome_arquivo)\n",
    "\n",
    "    # Construir URL completa\n",
    "    url_arquivo = urljoin(URL_BASE, href)\n",
    "\n",
    "    # Baixar o arquivo\n",
    "    try:\n",
    "        r = requests.get(url_arquivo, allow_redirects=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(caminho_arquivo, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"‚úÖ Arquivo salvo: {caminho_arquivo}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Erro HTTP {r.status_code}: {url_arquivo}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao baixar {url_arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fcbae-c9f5-4990-9853-96834a82c65b",
   "metadata": {},
   "source": [
    "## üìÑ Etapa 2 - Extra√ß√£o de Tabelas dos PDFs\n",
    "\n",
    "Ap√≥s a coleta automatizada dos arquivos PDF no site da ANVISA, esta etapa do pipeline tem como objetivo **extrair as tabelas contidas nos arquivos PDF**, tratando e estruturando os dados em formato tabular (Excel `.xlsx`).\n",
    "\n",
    "Dois m√©todos de extra√ß√£o s√£o utilizados, dependendo da estrutura do PDF:\n",
    "\n",
    "- **pdfplumber**: ideal para tabelas simples com separa√ß√µes baseadas em espa√ßamento.\n",
    "- **Camelot (modo `lattice`) + PyPDF2**: necess√°rio para PDFs com tabelas estruturadas por linhas desenhadas ou com formata√ß√£o irregular.\n",
    "\n",
    "---\n",
    "\n",
    "### üß∞ Ferramentas Utilizadas\n",
    "\n",
    "- **pdfplumber**: biblioteca para extra√ß√£o de tabelas de PDFs com estrutura tabular simples.\n",
    "- **Camelot**: utilizada para detectar tabelas baseadas em linhas desenhadas (modo `lattice`).\n",
    "- **PyPDF2**: leitura do n√∫mero de p√°ginas dos PDFs.\n",
    "- **pandas**: consolida√ß√£o e tratamento dos dados tabulados.\n",
    "- **Regex**: remo√ß√£o de caracteres ilegais para compatibilidade com Excel.\n",
    "- **gc**: libera√ß√£o manual da mem√≥ria ao fim do processamento de cada arquivo.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† L√≥gica dos Scripts\n",
    "\n",
    "#### üìò Etapa 2.1 ‚Äì Extra√ß√£o com `pdfplumber`\n",
    "\n",
    "1. **Varre recursivamente a pasta `dados/`**, onde os arquivos PDF foram salvos.\n",
    "2. Para cada arquivo:\n",
    "   - Identifica o **tipo do produto** (ex: `medicamentos`) e o **ano** a partir do nome do arquivo (ex: `medicamentos_2018.pdf`).\n",
    "   - Realiza a **extra√ß√£o das tabelas** contidas em todas as p√°ginas com `pdfplumber`.\n",
    "   - Cria um DataFrame consolidado contendo todas as tabelas extra√≠das.\n",
    "   - Adiciona uma nova coluna chamada `\"Tipo do Produto\"` com o valor leg√≠vel (`medicamentos`, `cosm√©ticos`, etc).\n",
    "3. **Remove caracteres ilegais** com regex para garantir compatibilidade com o Excel.\n",
    "4. Salva o resultado como `.xlsx` na pasta `dataframe/ano`.\n",
    "\n",
    "#### üìô Etapa 2.2 ‚Äì Extra√ß√£o com `Camelot` + `PyPDF2`\n",
    "\n",
    "1. Reprocessa todos os arquivos `.pdf` na pasta `dados/`, p√°gina por p√°gina.\n",
    "2. Para cada PDF:\n",
    "   - Usa `PyPDF2` para identificar o n√∫mero de p√°ginas.\n",
    "   - Tenta extrair tabelas usando `Camelot` com o modo `\"lattice\"`.\n",
    "   - Aplica fun√ß√µes de limpeza:\n",
    "     - Define corretamente o cabe√ßalho da tabela.\n",
    "     - Remove repeti√ß√µes de cabe√ßalhos entre p√°ginas.\n",
    "     - Corrige quebras de √≠ndice, caso n√∫mero e descri√ß√£o estejam juntos.\n",
    "   - Adiciona a coluna `\"Tipo do Produto\"` com base no nome do arquivo.\n",
    "   - Remove caracteres ilegais com regex.\n",
    "3. Salva o resultado final como `.xlsx` na pasta `dataframe/ano/`.\n",
    "4. Libera a mem√≥ria ao final do processamento de cada PDF.\n",
    "\n",
    "---\n",
    "\n",
    "### üßº Tratamento de Caracteres Inv√°lidos\n",
    "\n",
    "Certos PDFs podem conter caracteres invis√≠veis ou inv√°lidos (ex: `\\x0B`, `\\x0C`), que causam erro ao salvar em Excel. Ambos os scripts utilizam uma express√£o regular para limpar todas as colunas do tipo texto.\n",
    "\n",
    "```python\n",
    "ILLEGAL_EXCEL_CHARS = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Organiza√ß√£o da Sa√≠da\n",
    "\n",
    "A sa√≠da da extra√ß√£o (de ambos os m√©todos) √© salva em arquivos `.xlsx`, um por PDF, na pasta:\n",
    "\n",
    "```\n",
    "dataframe/ano/\n",
    "‚îú‚îÄ‚îÄ medicamentos_2011.xlsx\n",
    "‚îú‚îÄ‚îÄ cosmeticos_2013.xlsx\n",
    "‚îî‚îÄ‚îÄ alimentos_2018.xlsx\n",
    "```\n",
    "\n",
    "Cada planilha ter√° uma coluna adicional: `Tipo do Produto`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Resultado Esperado\n",
    "\n",
    "Arquivos Excel com os dados extra√≠dos e estruturados de forma padronizada, prontos para serem tratados e consolidados na pr√≥xima etapa do ETL.\n",
    "\n",
    "---\n",
    "\n",
    "### üö® Observa√ß√µes\n",
    "\n",
    "- Os dois scripts processam somente arquivos `.pdf`.\n",
    "- PDFs sem tabelas detect√°veis ser√£o ignorados.\n",
    "- Tipos de produto s√£o mapeados automaticamente a partir do nome do arquivo, com base no dicion√°rio:\n",
    "\n",
    "```python\n",
    "TIPOS_HUMAN = {\n",
    "    \"medicamentos\": \"medicamento\",\n",
    "    \"produtos_saude\": \"produto para sa√∫de\",\n",
    "    \"cosmeticos\": \"cosm√©tico\",\n",
    "    \"alimentos\": \"alimento\"\n",
    "}\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è **Importante:** Os arquivos `produtos_saude_2014.pdf` e `cosmeticos_2015.pdf` foram salvos com formata√ß√£o que impede a identifica√ß√£o correta das colunas pelo `pdfplumber`. \n",
    "> \n",
    "> Portanto, ap√≥s a convers√£o autom√°tica, ser√° necess√°rio **abrir manualmente os arquivos `.xlsx` gerados**, e:\n",
    ">\n",
    "> - Copiar a **estrutura de colunas de um arquivo semelhante** (ex: `medicamentos_2014.xlsx`, `medicamentos_2015.xlsx`)\n",
    "> - **Colar como cabe√ßalho** nos arquivos problem√°ticos.\n",
    ">\n",
    "> Isso garante que a padroniza√ß√£o e consolida√ß√£o nas etapas seguintes funcione corretamente.\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Ordem Recomendada de Execu√ß√£o\n",
    "\n",
    "1. **Execute primeiro o script com `pdfplumber`** para processar os arquivos com estrutura simples.\n",
    "2. **Depois, execute o script com `Camelot`**, que tentar√° reprocessar os arquivos restantes, corrigindo falhas do passo anterior.\n",
    "\n",
    "> Essa sequ√™ncia garante a m√°xima recupera√ß√£o dos dados e evita sobrescritas desnecess√°rias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54cb88-9705-4031-a375-ae55611be682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# === CONFIGURA√á√ïES ===\n",
    "DATA_DIR = \"dados\"\n",
    "OUTPUT_DIR = \"dataframe/ano\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Lista de arquivos desejados (sem extens√£o)\n",
    "arquivos_desejados = [\n",
    "    \"produtos_saude_2014\",\n",
    "    \"cosmeticos_2015\",\n",
    "    \"produtos_saude_2017\"\n",
    "]\n",
    "\n",
    "TIPOS_HUMAN = {\n",
    "    \"medicamentos\": \"Medicamento\",\n",
    "    \"produtos_saude\": \"Produto para Saude\",\n",
    "    \"cosmeticos\": \"Cosm√©tico\",\n",
    "    \"alimentos\": \"Alimento\"\n",
    "}\n",
    "\n",
    "ILLEGAL_EXCEL_CHARS = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]')\n",
    "\n",
    "def limpar_caracteres_invalidos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            if pd.api.types.is_object_dtype(df[col]):\n",
    "                df[col] = df[col].astype(str).str.replace(ILLEGAL_EXCEL_CHARS, \"\", regex=True)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao limpar coluna '{col}': {e}\")\n",
    "    return df\n",
    "\n",
    "# === PROCESSAMENTO DOS PDFs ===\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        slug = os.path.splitext(filename)[0]  # nome sem extens√£o\n",
    "        if filename.lower().endswith(\".pdf\") and slug in arquivos_desejados:\n",
    "            filepath = os.path.join(root, filename)\n",
    "            parts = slug.split(\"_\")\n",
    "\n",
    "            year = parts[-1]\n",
    "            tipo_slug = \"_\".join(parts[:-1])\n",
    "            tipo_human = TIPOS_HUMAN.get(tipo_slug, tipo_slug)\n",
    "\n",
    "            dfs = []\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    table = page.extract_table()\n",
    "                    if table:\n",
    "                        df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                        dfs.append(df)\n",
    "\n",
    "            df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "            df_all[\"Tipo do Produto\"] = tipo_human\n",
    "            df_all = limpar_caracteres_invalidos(df_all)\n",
    "\n",
    "            output_filename = f\"{slug}.xlsx\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            df_all.to_excel(output_path, index=False)\n",
    "\n",
    "            print(f\"‚úî Gerado: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897b3b3-deed-473b-909a-769804d87c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "import camelot\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# === CONFIGURA√á√ïES ===\n",
    "DATA_DIR = \"dados\"\n",
    "OUTPUT_DIR = \"dataframe/ano\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TIPOS_HUMAN = {\n",
    "    \"medicamentos\": \"Medicamento\",\n",
    "    \"produtos_saude\": \"Produto para Saude\",\n",
    "    \"cosmeticos\": \"Cosm√©tico\",\n",
    "    \"alimentos\": \"Alimento\"\n",
    "}\n",
    "\n",
    "ILLEGAL_EXCEL_CHARS = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]')\n",
    "\n",
    "# Arquivos a serem ignorados (sem extens√£o .pdf)\n",
    "ARQUIVOS_IGNORADOS = {\n",
    "    \"produtos_saude_2014\",\n",
    "    \"cosmeticos_2015\",\n",
    "    \"produtos_saude_2017\",\n",
    "    \"produtos_saude_2021\"\n",
    "}\n",
    "\n",
    "def limpar_caracteres_invalidos(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            if pd.api.types.is_object_dtype(df[col]):\n",
    "                df[col] = df[col].astype(str).str.replace(ILLEGAL_EXCEL_CHARS, \"\", regex=True)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao limpar coluna '{col}': {e}\")\n",
    "    return df\n",
    "\n",
    "def corrigir_indice_duplicado(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.shape[1] < 2:\n",
    "        return df\n",
    "\n",
    "    col0 = df.columns[0]\n",
    "    col1 = df.columns[1]\n",
    "    padrao = re.compile(r\"^(\\d{1,4})\\s+(.*)\")\n",
    "\n",
    "    for i in df.index:\n",
    "        val0 = str(df.at[i, col0]).strip() if pd.notna(df.at[i, col0]) else \"\"\n",
    "        val1 = str(df.at[i, col1]).strip() if pd.notna(df.at[i, col1]) else \"\"\n",
    "\n",
    "        match0 = padrao.match(val0)\n",
    "        match1 = padrao.match(val1)\n",
    "\n",
    "        if val0 == \"\" and match1:\n",
    "            numero, texto = match1.groups()\n",
    "            df.at[i, col0] = numero\n",
    "            df.at[i, col1] = texto\n",
    "        elif match1 and match1.group(1) == val0:\n",
    "            df.at[i, col1] = match1.group(2)\n",
    "        elif match0 and val1 == \"\":\n",
    "            numero, texto = match0.groups()\n",
    "            df.at[i, col0] = numero\n",
    "            df.at[i, col1] = texto\n",
    "\n",
    "    return df\n",
    "\n",
    "def padronizar_tabela(tabela: pd.DataFrame) -> pd.DataFrame:\n",
    "    tabela.columns = tabela.iloc[0]\n",
    "    tabela = tabela.drop(index=0).reset_index(drop=True)\n",
    "    tabela = tabela[~tabela.eq(tabela.columns).all(axis=1)]\n",
    "    tabela = corrigir_indice_duplicado(tabela)\n",
    "    return tabela\n",
    "\n",
    "# === PROCESSAMENTO DOS PDFs ===\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            slug = os.path.splitext(filename)[0]\n",
    "            if slug in ARQUIVOS_IGNORADOS:\n",
    "                print(f\"‚è≠ Ignorando arquivo: {filename}\")\n",
    "                continue\n",
    "\n",
    "            filepath = os.path.join(root, filename)\n",
    "            parts = slug.split(\"_\")\n",
    "\n",
    "            year = parts[-1]\n",
    "            tipo_slug = \"_\".join(parts[:-1])\n",
    "            tipo_human = TIPOS_HUMAN.get(tipo_slug, tipo_slug)\n",
    "\n",
    "            try:\n",
    "                reader = PdfReader(filepath)\n",
    "                num_pages = len(reader.pages)\n",
    "                dfs = []\n",
    "\n",
    "                for page_num in range(1, num_pages + 1):\n",
    "                    try:\n",
    "                        tables = camelot.read_pdf(filepath, pages=str(page_num), flavor=\"lattice\")\n",
    "                        for table in tables:\n",
    "                            df = padronizar_tabela(table.df)\n",
    "                            dfs.append(df)\n",
    "                    except Exception as e_page:\n",
    "                        print(f\"‚ö†Ô∏è Erro na p√°gina {page_num} de {filename}: {e_page}\")\n",
    "\n",
    "                df_all = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "                if not df_all.empty:\n",
    "                    df_all[\"Tipo do Produto\"] = tipo_human\n",
    "                    df_all = limpar_caracteres_invalidos(df_all)\n",
    "\n",
    "                    output_filename = f\"{slug}.xlsx\"\n",
    "                    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "                    df_all.to_excel(output_path, index=False)\n",
    "                    print(f\"‚úî Gerado: {output_path}\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Nenhuma tabela encontrada em {filename}\")\n",
    "\n",
    "                # === Libera√ß√£o de mem√≥ria ===\n",
    "                del reader, dfs, tables, df_all\n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao processar {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf85a8c-775e-40ec-ba5b-007709ba1c36",
   "metadata": {},
   "source": [
    "\n",
    "## üßπ Etapa 3 - Tratamento e Padroniza√ß√£o dos Arquivos Excel\n",
    "\n",
    "Ap√≥s a extra√ß√£o dos dados tabulares dos PDFs, esta etapa tem como objetivo **limpar, padronizar e consolidar os arquivos `.xlsx`** gerados anteriormente, garantindo consist√™ncia nas colunas e estrutura dos dados ao longo dos anos.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† L√≥gica do Script\n",
    "\n",
    "1. **Varre os arquivos da pasta `dataframe/ano/`**, agrupando os arquivos por ano com base no nome do arquivo.\n",
    "2. Para cada planilha:\n",
    "   - Remove colunas irrelevantes como `\"Ordem\"` e `\"Unnamed: 0\"`.\n",
    "   - **Normaliza os nomes das colunas** (remove asteriscos, espa√ßos duplicados, etc).\n",
    "   - **Renomeia colunas** de acordo com um dicion√°rio (`col_map`) para garantir padroniza√ß√£o (ex: `\"Produto\"` ‚Üí `\"PRODUTO\"`).\n",
    "   - **Remove colunas duplicadas**, caso existam.\n",
    "   - Garante a exist√™ncia da coluna `\"UF\"`; se estiver ausente, ela √© criada.\n",
    "   - Quando a cidade e o estado estiverem juntos na coluna `\"CIDADE\"` (ex: `\"ATIBAIA - SP\"` ou `\"Porto Alegre/RS\"`), o script separa os valores em duas colunas: `\"CIDADE\"` e `\"UF\"`.\n",
    "   - A separa√ß√£o √© realizada com regex inteligente, que identifica UFs no final da string, mesmo se precedidas por h√≠fen, barra, v√≠rgula ou seguidas de pontua√ß√£o.\n",
    "   - Converte a sigla da UF para o formato `\"Nome do Estado (UF)\"` (ex: `\"SP\"` ‚Üí `\"S√£o Paulo (SP)\"`).\n",
    "3. **Reorganiza as colunas** na ordem definida para facilitar an√°lises posteriores.\n",
    "4. **Consolida todos os arquivos de um mesmo ano** em um √∫nico DataFrame e exporta para a pasta `dataframe/tratado/`.\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Mapeamento de Colunas\n",
    "\n",
    "O script utiliza o seguinte dicion√°rio de padroniza√ß√£o para uniformizar os nomes das vari√°veis, independentemente das varia√ß√µes que aparecem nos arquivos:\n",
    "\n",
    "```python\n",
    "col_map = {\n",
    "    \"Medicamento\": \"PRODUTO\",\n",
    "    \"Produto Sa√∫de\": \"PRODUTO\",\n",
    "    \"Produto\": \"PRODUTO\",\n",
    "    \"Empresa\": \"EMPRESA\",\n",
    "    \"N¬∫ lote\": \"N_LOTE\",\n",
    "    \"N¬∫ Lote\": \"N_LOTE\",\n",
    "    \"N¬∫ de S√©rie/ Lote\": \"N_LOTE\",\n",
    "    \"Quantidade Roubada/Furtada/Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/Furtada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/ Furtada/Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/Furtada/ Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"N¬∫ Nota Fiscal\": \"N_NOTA_FISCAL\",\n",
    "    \"N¬∫ Boletim de Ocorr√™ncia\": \"N_BOLETIM_DE_OCORRENCIA\",\n",
    "    \"Data do roubo ou extravio\": \"DATA_DO_ROUBO_OU_EXTRAVIO\",\n",
    "    \"Local do roubo ou extravio\": \"CIDADE\",\n",
    "    \"Cidade\": \"CIDADE\",\n",
    "    \"UF\": \"UF\",\n",
    "    \"Tipo do Produto\": \"TIPO_DE_PRODUTO\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Organiza√ß√£o da Sa√≠da\n",
    "\n",
    "Os arquivos tratados e consolidados s√£o salvos por ano na pasta:\n",
    "\n",
    "```\n",
    "dataframe/tratado/\n",
    "‚îú‚îÄ‚îÄ 2011_tratado.xlsx\n",
    "‚îú‚îÄ‚îÄ 2012_tratado.xlsx\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Resultado Esperado\n",
    "\n",
    "Ao final da execu√ß√£o, os dados estar√£o estruturados, consistentes e prontos para as pr√≥ximas etapas de an√°lise, integra√ß√£o ou visualiza√ß√£o, com colunas padronizadas e sem duplica√ß√µes ou inconsist√™ncias regionais.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b005f24-6587-47ca-a833-a1c695199287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# === CONFIGURA√á√ïES ===\n",
    "INPUT_FOLDER = \"dataframe/ano\"\n",
    "OUTPUT_FOLDER = \"dataframe/tratado\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Mapeamento de colunas ap√≥s limpeza\n",
    "col_map = {\n",
    "    \"Medicamento\": \"PRODUTO\",\n",
    "    \"Produto Sa√∫de\": \"PRODUTO\",\n",
    "    \"Produto\": \"PRODUTO\",\n",
    "    \"Empresa\": \"EMPRESA\",\n",
    "    \"N¬∫ lote\": \"N_LOTE\",\n",
    "    \"N¬∫ Lote\": \"N_LOTE\",\n",
    "    \"N¬∫ de S√©rie/ Lote\": \"N_LOTE\",\n",
    "    \"Quantidade Roubada/Furtada/Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/Furtada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/ Furtada/Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"Quantidade Roubada/Furtada/ Extraviada\": \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"N¬∫ Nota Fiscal\": \"N_NOTA_FISCAL\",\n",
    "    \"N¬∫ Boletim de Ocorr√™ncia\": \"N_BOLETIM_DE_OCORRENCIA\",\n",
    "    \"Data do roubo ou extravio\": \"DATA_DO_ROUBO_OU_EXTRAVIO\",\n",
    "    \"Local do roubo ou extravio\": \"CIDADE\",\n",
    "    \"Cidade\": \"CIDADE\",\n",
    "    \"UF\": \"UF\",\n",
    "    \"Tipo do Produto\": \"TIPO_DE_PRODUTO\"\n",
    "}\n",
    "\n",
    "ordem_final = [\n",
    "    \"TIPO_DE_PRODUTO\", \"PRODUTO\", \"EMPRESA\", \"N_LOTE\",\n",
    "    \"QUANTIDADE_ROUBADA_FURTADA_EXTRAVIADA\",\n",
    "    \"N_BOLETIM_DE_OCORRENCIA\", \"N_NOTA_FISCAL\",\n",
    "    \"DATA_DO_ROUBO_OU_EXTRAVIO\", \"CIDADE\", \"UF\"\n",
    "]\n",
    "\n",
    "# Mapa UF ‚Üí Nome do estado\n",
    "UF_ESTADO = {\n",
    "    \"AC\": \"Acre\", \"AL\": \"Alagoas\", \"AP\": \"Amap√°\", \"AM\": \"Amazonas\", \"BA\": \"Bahia\",\n",
    "    \"CE\": \"Cear√°\", \"DF\": \"Distrito Federal\", \"ES\": \"Esp√≠rito Santo\", \"GO\": \"Goi√°s\",\n",
    "    \"MA\": \"Maranh√£o\", \"MT\": \"Mato Grosso\", \"MS\": \"Mato Grosso do Sul\", \"MG\": \"Minas Gerais\",\n",
    "    \"PA\": \"Par√°\", \"PB\": \"Para√≠ba\", \"PR\": \"Paran√°\", \"PE\": \"Pernambuco\", \"PI\": \"Piau√≠\",\n",
    "    \"RJ\": \"Rio de Janeiro\", \"RN\": \"Rio Grande do Norte\", \"RS\": \"Rio Grande do Sul\",\n",
    "    \"RO\": \"Rond√¥nia\", \"RR\": \"Roraima\", \"SC\": \"Santa Catarina\", \"SP\": \"S√£o Paulo\",\n",
    "    \"SE\": \"Sergipe\", \"TO\": \"Tocantins\"\n",
    "}\n",
    "estado_por_nome = {v.upper(): k for k, v in UF_ESTADO.items()}\n",
    "\n",
    "# Separar cidade e UF quando estiverem juntas\n",
    "def separar_cidade_uf(row):\n",
    "    cidade = str(row[\"CIDADE\"]) if pd.notna(row[\"CIDADE\"]) else \"\"\n",
    "    uf = row[\"UF\"]\n",
    "\n",
    "    # Procurar padr√£o que termina com UF, ignorando acentos e tra√ßos variados\n",
    "    match = re.search(r\"([A-Z]{2})(?=[^\\w]?[\\s\\-/)]?$)\", cidade.strip().upper())\n",
    "    if match:\n",
    "        uf_extraida = match.group(1)\n",
    "        cidade_sem_uf = re.split(r\"[\\s,;/\\-]+\" + uf_extraida + r\"[\\s)]*$\", cidade.strip(), maxsplit=1)[0]\n",
    "        cidade_formatada = cidade_sem_uf.title().strip()\n",
    "        return cidade_formatada, uf_extraida\n",
    "    return cidade.strip().title(), uf\n",
    "\n",
    "# Formatar UF como \"Estado (UF)\", aceitando siglas ou nomes\n",
    "def formatar_estado_completo(sigla):\n",
    "    if pd.isna(sigla):\n",
    "        return None\n",
    "    sigla = str(sigla).strip().upper()\n",
    "    sigla = unicodedata.normalize(\"NFKD\", sigla).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "    if sigla in UF_ESTADO:\n",
    "        return f\"{UF_ESTADO[sigla]} ({sigla})\"\n",
    "    elif sigla.title() in UF_ESTADO.values():\n",
    "        sigla_lookup = estado_por_nome.get(sigla.upper())\n",
    "        if sigla_lookup:\n",
    "            return f\"{UF_ESTADO[sigla_lookup]} ({sigla_lookup})\"\n",
    "    return None\n",
    "\n",
    "# Agrupar arquivos por ano\n",
    "arquivos_por_ano = {}\n",
    "for arquivo in os.listdir(INPUT_FOLDER):\n",
    "    if arquivo.endswith(\".xlsx\"):\n",
    "        ano_match = re.search(r\"(\\d{4})\", arquivo)\n",
    "        if ano_match:\n",
    "            ano = ano_match.group(1)\n",
    "            arquivos_por_ano.setdefault(ano, []).append(arquivo)\n",
    "\n",
    "# Processar cada ano\n",
    "for ano, arquivos in sorted(arquivos_por_ano.items()):\n",
    "    print(f\"\\nüìÜ Consolidando ano {ano}...\")\n",
    "    dfs = []\n",
    "\n",
    "    for arquivo in arquivos:\n",
    "        caminho = os.path.join(INPUT_FOLDER, arquivo)\n",
    "        print(f\"   üìÑ Processando: {arquivo}\")\n",
    "        try:\n",
    "            df = pd.read_excel(caminho)\n",
    "\n",
    "            df.columns = (\n",
    "                df.columns\n",
    "                .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                .str.replace(\"*\", \"\", regex=False)\n",
    "                .str.strip()\n",
    "            )\n",
    "\n",
    "            print(f\"   üîç Colunas normalizadas: {list(df.columns)}\")\n",
    "            df = df[[col for col in df.columns if col not in [\"Ordem\", \"Unnamed: 0\"]]]\n",
    "            df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n",
    "            duplicadas = df.columns[df.columns.duplicated()].tolist()\n",
    "            if duplicadas:\n",
    "                print(f\"   ‚ö†Ô∏è Colunas duplicadas removidas: {duplicadas}\")\n",
    "                df = df.loc[:, ~df.columns.duplicated()]\n",
    "            if \"UF\" not in df.columns:\n",
    "                df[\"UF\"] = None\n",
    "            if \"CIDADE\" in df.columns:\n",
    "                df[[\"CIDADE\", \"UF\"]] = df.apply(separar_cidade_uf, axis=1, result_type=\"expand\")\n",
    "            df[\"UF\"] = df[\"UF\"].apply(formatar_estado_completo)\n",
    "            colunas_existentes = [col for col in ordem_final if col in df.columns]\n",
    "            df = df[colunas_existentes]\n",
    "\n",
    "            print(f\"   ‚úÖ Colunas finais: {list(df.columns)}\")\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao processar {arquivo}: {e}\")\n",
    "\n",
    "    if dfs:\n",
    "        df_ano = pd.concat(dfs, ignore_index=True)\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, f\"{ano}_tratado.xlsx\")\n",
    "        df_ano.to_excel(output_path, index=False)\n",
    "        print(f\"üìÅ Arquivo salvo: {output_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Nenhum dado v√°lido encontrado para {ano}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a46d7-f02f-4932-8ab8-dfc861a87a9c",
   "metadata": {},
   "source": [
    "## üß© Etapa 4 - Consolida√ß√£o Final dos Dados\n",
    "\n",
    "Nesta etapa final, todos os arquivos `.xlsx` previamente tratados e estruturados s√£o **lidos, unificados e padronizados** em um √∫nico DataFrame, resultando na base final consolidada.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† L√≥gica do Script\n",
    "\n",
    "1. **L√™ todos os arquivos da pasta `dataframe/tratado/`**:\n",
    "   - Todos os arquivos `.xlsx` s√£o carregados e armazenados em uma lista de DataFrames.\n",
    "2. **L√™ tamb√©m arquivos Excel adicionais** que estejam presentes nas subpastas da pasta `dados/`, **exceto a pasta `dados/2023/`**, que √© explicitamente ignorada.\n",
    "3. **Concatena todos os DataFrames** coletados, removendo poss√≠veis duplicatas.\n",
    "4. **Padroniza os valores da coluna `TIPO_DE_PRODUTO`** removendo espa√ßos em branco e convertendo para string.\n",
    "5. **Salva o DataFrame final** no arquivo `consolidado_final.xlsx`, na pasta `dataframe/consolidado`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Padroniza√ß√£o de `TIPO_DE_PRODUTO`\n",
    "\n",
    "Caso a coluna `TIPO_DE_PRODUTO` esteja presente, seus valores s√£o convertidos para string e espa√ßos extras s√£o removidos com `.str.strip()`.  \n",
    "Se a coluna n√£o for encontrada, um aviso √© exibido no console, mas o processo continua normalmente.\n",
    "\n",
    "---\n",
    "\n",
    "### üìÅ Organiza√ß√£o da Sa√≠da\n",
    "\n",
    "O arquivo final √© salvo como:\n",
    "\n",
    "```\n",
    "dataframe/consolidado/consolidado_final.xlsx\n",
    "```\n",
    "\n",
    "Este arquivo cont√©m os dados de **todos os anos**, com colunas padronizadas e valores tratados, pronto para an√°lises explorat√≥rias, estat√≠sticas ou visualiza√ß√µes.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Resultado Esperado\n",
    "\n",
    "- Um √∫nico arquivo `.xlsx` consolidado, com todas as informa√ß√µes limpas, padronizadas e integradas.\n",
    "- Dados provenientes tanto dos arquivos tratados quanto dos arquivos adicionais dentro de `dados/`, exceto 2023.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce9ce9-8baa-416c-adc9-70634543868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Oculta avisos do openpyxl\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n",
    "\n",
    "# Caminhos\n",
    "PASTA_TRATADO = \"dataframe/tratado\"\n",
    "PASTA_DADOS = \"dados\"\n",
    "PASTA_EXCLUIDA = os.path.join(PASTA_DADOS, \"2023\")\n",
    "PASTA_OUTPUT = \"dataframe/consolidado\"\n",
    "os.makedirs(PASTA_OUTPUT, exist_ok=True)\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# === 1. Ler arquivos da pasta \"dataframe/tratado\" ===\n",
    "for arquivo in os.listdir(PASTA_TRATADO):\n",
    "    if arquivo.endswith(\".xlsx\"):\n",
    "        caminho_arquivo = os.path.join(PASTA_TRATADO, arquivo)\n",
    "        try:\n",
    "            df = pd.read_excel(caminho_arquivo, sheet_name=0, engine=\"openpyxl\")\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao ler {arquivo} em tratado: {e}\")\n",
    "\n",
    "# === 2. Ler arquivos .xlsx de todas as subpastas da pasta \"dados\", exceto \"dados/2023\" ===\n",
    "for raiz, _, arquivos in os.walk(PASTA_DADOS):\n",
    "    if os.path.commonpath([raiz, PASTA_EXCLUIDA]) == PASTA_EXCLUIDA:\n",
    "        continue  # pula a pasta 'dados/2023' e suas subpastas\n",
    "    for arquivo in arquivos:\n",
    "        if arquivo.endswith(\".xlsx\"):\n",
    "            caminho_arquivo = os.path.join(raiz, arquivo)\n",
    "            try:\n",
    "                df = pd.read_excel(caminho_arquivo, sheet_name=0, engine=\"openpyxl\")\n",
    "                dataframes.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao ler {arquivo} em dados: {e}\")\n",
    "\n",
    "# === 3. Concatenar e remover duplicatas ===\n",
    "if dataframes:\n",
    "    df_consolidado = pd.concat(dataframes, ignore_index=True)\n",
    "    df_consolidado = df_consolidado.drop_duplicates()\n",
    "\n",
    "    # Apenas remove espa√ßos e coloca em min√∫sculo se quiser algum padr√£o b√°sico\n",
    "    if \"TIPO_DE_PRODUTO\" in df_consolidado.columns:\n",
    "        df_consolidado[\"TIPO_DE_PRODUTO\"] = df_consolidado[\"TIPO_DE_PRODUTO\"].astype(str).str.strip()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Coluna 'TIPO_DE_PRODUTO' n√£o encontrada.\")\n",
    "\n",
    "    # Salvar resultado\n",
    "    output_path = os.path.join(PASTA_OUTPUT, \"consolidado_final.xlsx\")\n",
    "    df_consolidado.to_excel(output_path, index=False)\n",
    "    print(f\"‚úÖ Consolida√ß√£o final salva em: {output_path}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum arquivo .xlsx encontrado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
